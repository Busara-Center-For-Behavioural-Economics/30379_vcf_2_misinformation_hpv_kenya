---
title: "VCF2 - Sampling"
author: "Jonathan Karl"
date: '2023-04-19'
output: html_document
---

```{r}
rm(list = ls())

# Prevent scientific notation
knitr::opts_knit$set(options(scipen=999))

# Install and load all the packages that will be used for analysis
pkgs <- c("tidyverse", "sp", "googlesheets4","rgdal", "rgeos", "raster", "sf", "randomizr")

miss_pkgs <- pkgs[!pkgs %in% installed.packages()[,1]] # vector of missing packages
if(length(miss_pkgs)>0){
  install.packages(miss_pkgs)
}
invisible(lapply(pkgs,library,character.only=TRUE))

# Clear our memory by removing objects that are no longer needed.
rm(miss_pkgs, pkgs)
```

*From the Paper No margin, no mission? A field experiment on incentives for public service delivery:* To implement the design, we first conducted a census of all hair salons in Lusaka, collecting GPS coordinates and numerous salon and stylist characteristics. We then imposed a grid on the GPS-mapped locations of the salons, to divide the city into equal geographical areas of 650 by 650 m each. We excluded a buffer of 75 m on all sides of the grid cell, resulting in at least 150 m between salons in adjacent areas. The resulting areas, each measuring 250,000 m2, served as the unit of randomization. Salons located in buffer areas were not invited to join the program. The final sample for randomization consists of 205 distinct neighbourhoods, containing 1222 hair salons.

*Jonathan's Approach:* This study is design for urban and peri-urban areas in and around Nairobi. Accordingly, we labelled all wards in Nairobi and surrounding counties (Kiambu, Machakos, Kajiado) with the help of Field Officers as rural, peri-urban and urban. Within the wards labelled as peri-urban and urban, we randomly allocated circular clusters with a 1-mile radius (the minimum geographical targeting area Facebook's advertising ecosystem allows). 

# Sampling Clusters

## 1. Read in geo-data and geo-entity labels

```{r}

# Read in Shapefile
shp_kenya_adm3 <- readOGR(dsn = "Raw Data/gadm41_KEN_shp/gadm41_KEN_3.shp")

# Select relevant counties
shp_kenya_adm3 <- shp_kenya_adm3[shp_kenya_adm3$NAME_1 %in% c("Nairobi", "Machakos", "Kajiado", "Kiambu"),]

# Select relevant Wards
labelled_wards <- read_sheet(ss = "https://docs.google.com/spreadsheets/d/1NLDeLVP5OXw91kX41BCtgZmZnjIsSZfTBjmXBsSnqFs/edit#gid=1092120316")

# Create final label for wards without disagreement
idx_no_disagreement <- apply(labelled_wards[,4:7], FUN = function(x) length(unique(na.omit(x))) < 2, MARGIN = 1)
labelled_wards$final_label[idx_no_disagreement] <- apply(labelled_wards[idx_no_disagreement,4:7], FUN = function(x) unique(na.omit(x)), MARGIN = 1)

# For those with disagreement, adopt the field verification column
labelled_wards$final_label[!idx_no_disagreement] <- labelled_wards$`Not-rural? (i.e. urban / peri-urban) - Field verification`[!idx_no_disagreement]

# Add the last few in Kiambu
labelled_wards$final_label[!is.na(labelled_wards$`Not-rural? (i.e. urban / peri-urban) - Busara colleagues`)] <- labelled_wards$`Not-rural? (i.e. urban / peri-urban) - Busara colleagues`[!is.na(labelled_wards$`Not-rural? (i.e. urban / peri-urban) - Busara colleagues`)]

# Export Shapefile to view in QGIS for sensechecking output visually
shp_kenya_adm3$geo_entity <- labelled_wards$final_label
writeOGR(shp_kenya_adm3, dsn = 'gadm41_KEN_shp', layer = 'gadm41_KEN_3_urban_rural_classified', driver = "ESRI Shapefile", overwrite_layer = TRUE)

```

## 2. Distribute points

```{r}

## Select only polygons that are urban or peri-urban
#shp_kenya_adm3 <- shp_kenya_adm3[shp_kenya_adm3$geo_entity %in% c("urban", "peri-urban"),]
#
## Define the bounds of the polygon underlying the geospatial object
#bounds <- shp_kenya_adm3@polygons[[1]]@Polygons[[1]]@coords
#
## Define the distance threshold (2 miles in this case)
#distance_threshold <- 2 * 1609.34 + 200  # 1 mile = 1609.34 meters + Buffer between Clusters
#
## Create an empty spatial points object
#points <- SpatialPoints(data.frame(x = 0, y = 0), proj4string = CRS(proj4string(shp_kenya_adm3)))[-1,]
#
## Set a seed
#set.seed(92)
#
## Generate random points until reaching the desired number (100 in this case)
#count_points <- 1
#count_tries <- 1
#while(length(points) < 1000) {
#  # Generate a random point within the bounds of the polygon
#  random_point <- spsample(shp_kenya_adm3, 1, type = "random", iter=20)
#  #print(random_point)
#  print(count_tries)
#  
#  # Check if the new point is at least 2 miles away from existing points
#  if(length(points) > 0) {
#    distances <- raster::pointDistance(points, random_point, lonlat = T)
#    count_tries <- count_tries + 1
#
#    if(min(distances) < distance_threshold) {
#      next  # Skip to the next iteration if the minimum distance is less than the threshold
#    }
#  }
#  
#  # Add the new point to the points object
#  points <- rbind(points, random_point)
#  print(paste("New Point:", count_points))
#  count_points <- count_points + 1
#}
#
## Plot the points within the polygon
#plot(shp_kenya_adm3)
#points(points, col = "red", pch = 1)
#
## Export
#points_df <- SpatialPointsDataFrame(coords = data.frame(points), data = data.frame(cluster_id = 1:length(points)))
#temp <- as(points,"SpatialPointsDataFrame")
#writeOGR(obj=points_df, dsn="clusters", layer="points", driver="ESRI Shapefile", overwrite_layer = TRUE) # this is in equal area projection

```

## 3. Select clusters with a population probability weight

```{r}

# How many clusters? (100 necessary for power calculation --> but we will oversample by 16%)
number_clusters <- 116

# Read in Population File (Takes some time...the commented out code crops the file to the study areas and cuts out the rest of Kenya)
#ken_general_2020 <- read.csv("population_density_data/ken_general_2020.csv")
#head(ken_general_2020)
#
## Filter df for the rows within the box of the 4 target counties
#ken_general_2020_cropped <- ken_general_2020 %>% 
#  filter(longitude >= 35.9962005610001370 & longitude <= 37.9374580390000347 & latitude >= -3.1870117179998942 & latitude <= -0.7526243919999729)
#write.csv(ken_general_2020_cropped, "population_density_data/ken_general_2020_cropped.csv")

# Read in the filtered data
ken_general_2020_cropped <- read.csv("Raw Data/population_density_data/ken_general_2020_cropped.csv")

# Convert to sf object
ken_general_2020_cropped_sf <- st_as_sf(ken_general_2020_cropped, coords = c("longitude", "latitude"), crs = 4326)

# Read in cluster points
cluster_choiceset <- readOGR("clusters/points.shp")
crs(cluster_choiceset) <- "+proj=longlat +datum=WGS84 +no_defs"

# Convert to sf data frame
clusters_sf <- st_as_sf(cluster_choiceset)

# Set the radius in meters (1 mile = 1609.34 meters)
radius_meters <- 1609.34

# Create circular polygons with a radius of 1 mile
clusters_circles_sf <- st_buffer(clusters_sf, dist = radius_meters)

# Join clusters and population_points
joined_data <- st_join(ken_general_2020_cropped_sf, clusters_circles_sf, join = st_within)

# Aggregate the sum of the population density for each polygon (cluster)
aggregated_pop_cluster <- aggregate(joined_data$ken_general_2020, list(joined_data$cluster_id), sum)

#### Plot the clusters onto the map with population density and administrative boundaries #####

### To enable the plotting of the pop-density data, convert joined geodata into df and clean it
joined_data_df <- data.frame(joined_data)
joined_data_df$geometry <- as.character(joined_data_df$geometry)
joined_data_df_temp <- joined_data_df %>% 
  separate(geometry, into = c("Longitude", "Latitude"), ",")
joined_data_df_temp$Longitude <- as.numeric(str_remove_all(joined_data_df_temp$Longitude, "c\\("))
joined_data_df_temp$Latitude <- as.numeric(str_remove_all(joined_data_df_temp$Latitude, "\\)"))

# To enable plotting the clusters and their associated population density --> Merge the aggregated population data and the locations of the clusters and select the clusters by population weight
aggregated_pop_cluster <- merge(data.frame(cluster_choiceset), aggregated_pop_cluster, by.x = "cluster_id", by.y = "Group.1")

################ Randomly select clusters with population weights
# Set a seed
set.seed(92)

# Sample
selected_cluster_ids <- sample(x = aggregated_pop_cluster$cluster_id, prob = aggregated_pop_cluster$x, size = number_clusters, replace = F)

# Export clusters to look at them in QGIS
selected_clusters <- clusters_sf[clusters_sf$cluster_id %in% selected_cluster_ids,]
st_write(selected_clusters, "clusters/randomly_selected_clusters.shp", append = F)

# Export buffered clusters as well 
selected_clusters_buffered <- clusters_circles_sf[clusters_circles_sf$cluster_id %in% selected_cluster_ids,]
st_write(selected_clusters_buffered, "clusters/randomly_selected_clusters_buffered.shp", append = F)

```

## 4. Replace anticipated logistically unfeasible clusters

```{r}

############################ Re-Sampling 1

fo_assessment_data_collection_issues_by_cluster <- read_sheet("https://docs.google.com/spreadsheets/d/1qjiBQzsF3KVVPh0eQH8X3fhCrPaAp-TIUeWC7yNTZ5I/edit#gid=1915233803", sheet = "[old] Clusters Probability Data Collecton Issues")

# Extracting all cluster IDs that will likely face data collection issues.
cluster_ids_issue_identified_v1 <- fo_assessment_data_collection_issues_by_cluster$cluster_id[fo_assessment_data_collection_issues_by_cluster$Final_Assessment == "Possible issue"]

# Resample them
idx_eligible_for_resample <- !(aggregated_pop_cluster$cluster_id %in% selected_cluster_ids)
target_clusters_n <- 108
number_to_resample <- target_clusters_n - (sum(!idx_eligible_for_resample) - length(cluster_ids_issue_identified_v1))

# Set a seed
set.seed(10)

# Resample
new_selected_cluster_ids_v2 <- sample(x = aggregated_pop_cluster$cluster_id[idx_eligible_for_resample], prob = aggregated_pop_cluster$x[idx_eligible_for_resample], size = number_to_resample, replace = F)

temp <- clusters_circles_sf[clusters_circles_sf$cluster_id %in% new_selected_cluster_ids_v2,]
st_write(temp, "clusters/sample_v2.shp", append = F)


# Create a new vector outlining the currently selected clusters
logistically_feasible_clusters <- selected_cluster_ids[!(selected_cluster_ids %in% cluster_ids_issue_identified_v1)]
selected_cluster_ids_v2 <- sort(c(new_selected_cluster_ids_v2, logistically_feasible_clusters))

############################ Final sampled Clusters

# Create a new vector outlining the final selected clusters
logistically_infeasible_clusters <- c(cluster_ids_issue_identified_v1)
logistically_feasible_clusters <- selected_cluster_ids[!(selected_cluster_ids %in% logistically_infeasible_clusters)]
selected_cluster_ids_v2 <- sort(c(logistically_feasible_clusters, new_selected_cluster_ids_v2))

# Export final buffered clusters 
selected_clusters_buffered_v2 <- clusters_circles_sf[clusters_circles_sf$cluster_id %in% selected_cluster_ids_v2,]
st_write(selected_clusters_buffered_v2, "clusters/randomly_selected_clusters_buffered_v2.shp", append = F)

```


## 5. Allocating clusters to Treatment Groups

```{r}

# Read in buffered clusters
selected_buffered_clusters <- st_read("clusters/randomly_selected_clusters_buffered_v2.shp")

# Randomly allocate clusters to treatment groups
set.seed(4)
selected_buffered_clusters$group <- complete_ra(nrow(selected_buffered_clusters), num_arms = 4, conditions = c("Control", "Online_Only", "Offline_Only", "Online_Offline"))

# Export new Shapefile
st_crs(selected_buffered_clusters) <- st_crs(st_read("Raw Data/gadm41_KEN_shp/gadm41_KEN_3_urban_rural_classified.shp"))
st_write(selected_buffered_clusters, "clusters/randomly_selected_clusters_buffered_allocated_to_groups.shp", append = F)

# Also export csv
final_clusters_allocated_df <- data.frame(selected_buffered_clusters)[,c("cluster_id", "group")]
write.csv(final_clusters_allocated_df, "Data Exports/vcf_final_clusters_allocated.csv")
```

## 6. Adding potential clusters to falsely labelled areas 

```{r}

# # Read in the clusters to prevent cluster ID duplicates
# max_cluster_id_previous <- as.numeric(max(st_read("clusters/points.shp")$cluster_id))
# 
# ## Select only polygons that are urban or peri-urban
# shp_kenya_adm3_additions <- shp_kenya_adm3[shp_kenya_adm3$NAME_3 %in% c("Olkeri", "Kaputiei North", "Keekonyokie", "Dalalekutuk", "Ildamat") # & shp_kenya_adm3$NAME_1 == "Kajiado",]
#  
#  # Define the bounds of the polygon underlying the geospatial object
#  bounds <- shp_kenya_adm3_additions@polygons[[1]]@Polygons[[1]]@coords
#  
#  # Define the distance threshold (2 miles in this case)
#  distance_threshold <- 2 * 1609.34 + 200  # 1 mile = 1609.34 meters + Buffer between Clusters
#  
#  # Create an empty spatial points object
#  points <- SpatialPoints(data.frame(x = 0, y = 0), proj4string = CRS(proj4string(shp_kenya_adm3)))[-1,]
#  
#  # Set a seed
#  set.seed(92)
#  
#  # Generate random points until reaching the desired number (100 in this case)
#  count_points <- 1
#  count_tries <- 1
#  while(length(points) < 1000) {
#    # Generate a random point within the bounds of the polygon
#    random_point <- spsample(shp_kenya_adm3_additions, 1, type = "random", iter=20)
#    #print(random_point)
#    print(count_tries)
#    
#    # Check if the new point is at least 2 miles away from existing points
#    if(length(points) > 0) {
#      distances <- raster::pointDistance(points, random_point, lonlat = T)
#      count_tries <- count_tries + 1
#  
#      if(min(distances) < distance_threshold) {
#        next  # Skip to the next iteration if the minimum distance is less than the threshold
#      }
#    }
#    
#    # Add the new point to the points object
#    points <- rbind(points, random_point)
#    print(paste("New Point:", count_points))
#    count_points <- count_points + 1
#  }
#  
#  # Plot the points within the polygon
#  plot(shp_kenya_adm3)
#  points(points, col = "red", pch = 1)
#  
#  
#  # Export
#  points_df <- SpatialPointsDataFrame(coords = data.frame(points), data = data.frame(cluster_id = seq(from = max_cluster_id_previous+1, by = # 1, length.out = length(points))))
# writeOGR(obj=points_df, dsn="clusters", layer="additional_points", driver="ESRI Shapefile", overwrite_layer = TRUE) # this is in equal area # # projection

```

## 7. Moving & Replacing logistically actually difficult clusters

```{r}
move_cluster <- function(cluster_id, longitude, latitude){
  
  # Create new point acting as new cluster center
  temp_new_cluster_center <- st_sf(geometry = st_sfc(st_point(c(longitude, latitude)), crs = st_crs(clusters)))
  
  # Buffer the new cluster center to be a circle
  temp_new_cluster <- st_buffer(temp_new_cluster_center, dist = radius_meters)
  
  # Add the circle to replace the geometry of the cluster with "cluster_id"
  clusters[clusters$cluster_id == cluster_id,]$geometry <<- temp_new_cluster$geometry
}
```


### 7.1 - V1

```{r}

# Read in shapefiles and population density csv
additional_points <- st_read("clusters/additional_points.shp")
clusters <- st_read("clusters/randomly_selected_clusters_buffered_allocated_to_groups.shp")
all_initial_potential_clusters <- st_read("clusters/points.shp")
ken_general_2020_cropped <- read.csv("Raw Data/population_density_data/ken_general_2020_cropped.csv")
# Convert to csv to sf object
ken_general_2020_cropped_sf <- st_as_sf(ken_general_2020_cropped, coords = c("longitude", "latitude"), crs = 4326)

# Set the radius in meters (1 mile = 1609.34 meters)
radius_meters <- 1609.34


############################## 1. Moving Clusters

# Machakos
move_cluster(345, 37.442614, -1.461225) # Cluster 345
move_cluster(128, 37.369157, -1.315872) # Cluster 128
move_cluster(186, 37.011971, -1.413045) # Cluster 186
move_cluster(316, 37.271765, -1.222607) # Cluster 316
move_cluster(425, 37.343927, -1.369312) # Cluster 425
move_cluster(271, 37.340340, -1.288964) # Cluster 271
move_cluster(192, 37.132348, -1.532216) # Cluster 192

# Kiambu
move_cluster(129, 36.797578, -1.134905) # Cluster 129

################################ 2. Sampling New Clusters

# Merge 
all_current_potential_cluster <- rbind(all_initial_potential_clusters, additional_points)
st_crs(all_current_potential_cluster) <- 4326 # Set CRS properly

# Create circular polygons with a radius of 1 mile
all_current_potential_cluster_buffered <- st_buffer(all_current_potential_cluster, dist = radius_meters)

# Remove all additional points that overlap with old ones
potential_cluster_overlap <- st_overlaps(all_current_potential_cluster_buffered)
overlap_idx <- which(sapply(potential_cluster_overlap, function(i) any(i >= length(1)))) # Which idx are overlapping
cluster_idx_to_remove <- overlap_idx[overlap_idx > max(all_initial_potential_clusters$cluster_id)]
all_current_potential_cluster_buffered <- all_current_potential_cluster_buffered[-cluster_idx_to_remove,]

# Export to look at
st_write(all_current_potential_cluster_buffered, "clusters/points_and_additional_points.shp", append = F) 

# Join clusters and population_points
joined_data <- st_join(ken_general_2020_cropped_sf, all_current_potential_cluster_buffered, join = st_within)

# Aggregate the sum of the population density for each polygon (cluster)
aggregated_pop_cluster <- aggregate(joined_data$ken_general_2020, list(joined_data$cluster_id), sum)


##################### Sample new ones for abandoned ones V1

# Burned IDs --> Filter data for only eligible clusters to be re-sampled
burned_cluster_ids <- read.csv("clusters/burned_ids.csv")[,-1]$cluster_id
aggregated_pop_cluster_not_burned <- aggregated_pop_cluster[!(aggregated_pop_cluster$Group.1 %in% burned_cluster_ids),]

# Abandon the following clusters --> Machakos: 360, 37, 197 / Kajiado: 248 / Kiambu: 281 == 5 clusters
cluster_ids_remove <- c(360, 37, 197, 248, 281)

# Set a seed
set.seed(65927)

# Sample 
selected_cluster_ids <- sample(x = aggregated_pop_cluster_not_burned$Group.1, prob = aggregated_pop_cluster_not_burned$x, size = length(cluster_ids_remove), replace = F)
# Newly added: 364  33 141  46 432

# Now, replace the ID and geography of the 5 clusters that are to be removed in the "clusters" object with the new ones
cluster_idx_replace <- which(clusters$cluster_id %in% cluster_ids_remove)

# Extract polygons of new clusterIDs
selected_clusters_new <- all_current_potential_cluster_buffered[all_current_potential_cluster_buffered$cluster_id %in% selected_cluster_ids,]

# Re-Assign the cluster IDs and Geographies
clusters[cluster_idx_replace,]$cluster_id <- selected_clusters_new$cluster_id
clusters[cluster_idx_replace,]$geometry <- selected_clusters_new$geometry

# Export
st_write(clusters, "clusters/randomly_selected_clusters_buffered_allocated_to_groups_v2.shp", append = F)
# Also export csv
final_clusters_allocated_df <- data.frame(clusters)[,c("cluster_id", "group")]
write.csv(final_clusters_allocated_df, "Data Exports/vcf_final_clusters_allocated.csv")


read.csv("Data Exports/vcf_final_clusters_allocated.csv")

```

### 7.2 - V2

```{r}

# Read in shapefiles and population density csv
additional_points <- st_read("clusters/additional_points.shp")
clusters <- st_read("clusters/randomly_selected_clusters_buffered_allocated_to_groups_v2.shp")
all_initial_potential_clusters <- st_read("clusters/points.shp")
ken_general_2020_cropped <- read.csv("Raw Data/population_density_data/ken_general_2020_cropped.csv")
# Convert to csv to sf object
ken_general_2020_cropped_sf <- st_as_sf(ken_general_2020_cropped, coords = c("longitude", "latitude"), crs = 4326)

# Read burned cluster_ids and add update the file based on the latest clusters
burned_ids <- read.csv("clusters/burned_ids.csv")[,-1]
burned_ids <- rbind(burned_ids, data.frame(cluster_id = clusters$cluster_id, group = clusters$group))
burned_ids <- burned_ids[!duplicated(burned_ids),]


# Set the radius in meters (1 mile = 1609.34 meters)
radius_meters <- 1609.34

############################## 1. Moving Clusters

# Kiambu
move_cluster(325, 36.85145, -1.17010) # Cluster 325
move_cluster(444, 36.957456, -1.148411) # Cluster 444
move_cluster(55, 36.607520, -0.9480321) # Cluster 55
move_cluster(416, 36.62345, -0.89689) # Cluster 416
move_cluster(443, 36.614437, -0.843287) # Cluster 443
move_cluster(334, 36.652807, -1.114576) # Cluster 334
move_cluster(218, 36.716431, -1.171960) # Cluster 218
move_cluster(201, 36.791914, -1.096677) # Cluster 201

# Kajiado
move_cluster(39, 36.763241, -1.389647) # Cluster 39
move_cluster(370, 36.730253, -1.377878) # Cluster 370

# Machakos
move_cluster(432, 37.561393, -1.421066) # Cluster 432


# Nairobi
move_cluster(283, 36.816093, -1.247868) # Cluster 283

################################ 2. Sampling New Clusters

# Merge 
all_current_potential_cluster <- rbind(all_initial_potential_clusters, additional_points)
st_crs(all_current_potential_cluster) <- 4326 # Set CRS properly

# Create circular polygons with a radius of 1 mile
all_current_potential_cluster_buffered <- st_buffer(all_current_potential_cluster, dist = radius_meters)

# Remove all additional points that overlap with old ones
potential_cluster_overlap <- st_overlaps(all_current_potential_cluster_buffered)
overlap_idx <- which(sapply(potential_cluster_overlap, function(i) any(i >= length(1)))) # Which idx are overlapping
cluster_idx_to_remove <- overlap_idx[overlap_idx > max(all_initial_potential_clusters$cluster_id)]
all_current_potential_cluster_buffered <- all_current_potential_cluster_buffered[-cluster_idx_to_remove,]

# Export to look at
st_write(all_current_potential_cluster_buffered, "clusters/points_and_additional_points.shp", append = F) 

# Join clusters and population_points
joined_data <- st_join(ken_general_2020_cropped_sf, all_current_potential_cluster_buffered, join = st_within)

# Aggregate the sum of the population density for each polygon (cluster)
aggregated_pop_cluster <- aggregate(joined_data$ken_general_2020, list(joined_data$cluster_id), sum)


##################### Sample new ones for abandoned ones V2

# Burned IDs --> Filter data for only eligible clusters to be re-sampled
aggregated_pop_cluster_not_burned <- aggregated_pop_cluster[!(aggregated_pop_cluster$Group.1 %in% burned_ids),]

# Abandon the following clusters --> Machakos: 186, 414, 141 / Nairobi: 10, 363
cluster_ids_remove <- c(186, 414, 141, 10, 363)
selected_cluster_ids <- c(601, 605, 126, 622, 117)

# Now, replace the ID and geography of the 5 clusters that are to be removed in the "clusters" object with the new ones
cluster_idx_replace <- which(clusters$cluster_id %in% cluster_ids_remove)

# Extract polygons of new clusterIDs
selected_clusters_new <- all_current_potential_cluster_buffered[all_current_potential_cluster_buffered$cluster_id %in% selected_cluster_ids,]

# Re-Assign the cluster IDs and Geographies
clusters[cluster_idx_replace,]$cluster_id <- selected_clusters_new$cluster_id
clusters[cluster_idx_replace,]$geometry <- selected_clusters_new$geometry

# Slightly adjust the new clusters 
move_cluster(605, 36.68635, -1.43192) # Cluster 605
move_cluster(601, 36.684701, -1.396761) # Cluster 601
move_cluster(126, 36.635812, -1.283102) # Cluster 126
move_cluster(622, 36.84696, -1.68796) # Cluster 622
move_cluster(117, 36.583524, -1.151523) # Cluster 117

# Export
st_write(clusters, "clusters/randomly_selected_clusters_buffered_allocated_to_groups_v3.shp", append = F)
# Also export csv
final_clusters_allocated_df <- data.frame(clusters)[,c("cluster_id", "group")]
write.csv(final_clusters_allocated_df, "Data Exports/vcf_final_clusters_allocated.csv")
final_clusters_allocated_df

```


### 7.3 - V3

```{r}

# Read in shapefiles and population density csv
additional_points <- st_read("clusters/additional_points.shp")
clusters <- st_read("clusters/randomly_selected_clusters_buffered_allocated_to_groups_v3.shp")
all_initial_potential_clusters <- st_read("clusters/points.shp")
ken_general_2020_cropped <- read.csv("Raw Data/population_density_data/ken_general_2020_cropped.csv")
# Convert to csv to sf object
ken_general_2020_cropped_sf <- st_as_sf(ken_general_2020_cropped, coords = c("longitude", "latitude"), crs = 4326)

# Read burned cluster_ids and add update the file based on the latest clusters
burned_ids <- read.csv("clusters/burned_ids.csv")[,-1]
burned_ids <- rbind(burned_ids, data.frame(cluster_id = clusters$cluster_id, group = clusters$group))
burned_ids <- burned_ids[!duplicated(burned_ids),]


# Set the radius in meters (1 mile = 1609.34 meters)
radius_meters <- 1609.34

############################## 1. Moving Clusters

# Kiambu
move_cluster(103, 36.634734, -0.969261) # Cluster 325
move_cluster(444, 36.957456, -1.148411) # Cluster 444

# Machakos
move_cluster(192, 37.132402, -1.542821) # Cluster 192
move_cluster(316, 37.274071, -1.229091) # Cluster 316

# Nairobi
move_cluster(110, 37.014059, -1.283610) # Cluster 110

################################ 2. Sampling New Clusters

# Import all eligible points
all_current_potential_cluster_buffered <- st_read("clusters/points_and_additional_points.shp") 

# Join clusters and population_points
joined_data <- st_join(ken_general_2020_cropped_sf, all_current_potential_cluster_buffered, join = st_within)

# Aggregate the sum of the population density for each polygon (cluster)
aggregated_pop_cluster <- aggregate(joined_data$ken_general_2020, list(joined_data$cluster_id), sum)


##################### Sample new ones for abandoned ones V2

# Burned IDs --> Filter data for only eligible clusters to be re-sampled
aggregated_pop_cluster_not_burned <- aggregated_pop_cluster[!(aggregated_pop_cluster$Group.1 %in% burned_ids),]

# Abandon the following clusters --> Machakos: 186, 414, 141 / Nairobi: 10, 363
cluster_ids_remove <- c(378, 54)
selected_cluster_ids <- c(180, 261)

# Now, replace the ID and geography of the 5 clusters that are to be removed in the "clusters" object with the new ones
cluster_idx_replace <- which(clusters$cluster_id %in% cluster_ids_remove)

# Extract polygons of new clusterIDs
selected_clusters_new <- all_current_potential_cluster_buffered[all_current_potential_cluster_buffered$cluster_id %in% selected_cluster_ids,]

# Re-Assign the cluster IDs and Geographies
clusters[cluster_idx_replace,]$cluster_id <- selected_clusters_new$cluster_id
clusters[cluster_idx_replace,]$geometry <- selected_clusters_new$geometry

# Slightly adjust the new clusters 
move_cluster(180, 37.689700, -1.408045) # Cluster 180
move_cluster(261, 36.951790, -1.112885) # Cluster 261

# Export
st_write(clusters, "clusters/randomly_selected_clusters_buffered_allocated_to_groups_v4.shp", append = F)
# Also export csv
final_clusters_allocated_df <- data.frame(clusters)[,c("cluster_id", "group")]
write.csv(final_clusters_allocated_df, "Data Exports/vcf_final_clusters_allocated.csv")
final_clusters_allocated_df

```

