---
title: "HFC_VCF"
author: "Jonathan Karl"
date: '2023-09-13'
output: html_document
---

# Setup

```{r}

# Clean the environment
rm(list = ls())

# Prevent scientific notation
knitr::opts_knit$set(options(scipen=999))

# Install and load all the packages that will be used for analysis
pkgs <- c("tidyverse", "googlesheets4", "lubridate", "sp", "sf", "fishmethods", "WebPower", "geosphere", "RCT", "forecast", "plotly", "ltm")

miss_pkgs <- pkgs[!pkgs %in% installed.packages()[,1]] # vector of missing packages
if(length(miss_pkgs)>0){
  install.packages(miss_pkgs)
}
invisible(lapply(pkgs,library,character.only=TRUE))

# Clear our memory by removing objects that are no longer needed.
rm(miss_pkgs, pkgs)
```

## Helper Functions

```{r}

tblFun <- function(x){
  tbl <- table(x)
  res <- cbind(tbl,round(prop.table(tbl)*100,2))
  colnames(res) <- c('Count','Percentage')
  res
}

tblFun_II <- function(x, y){
  tbl <- table(x, y)
  res <- apply(tbl, MARGIN = 2, FUN = function(x) round(prop.table(x)*100,2))
  res
}

```

## Read SurveyCTO sheet

```{r}
surveycto_choices <- read_sheet("https://docs.google.com/spreadsheets/d/1hK-q9SLPZxPDDyrhv8PWn11wehqlXzwXzTsjUpOupb0/edit#gid=125397872", "choices")
survey_cto_survey <- read_sheet("https://docs.google.com/spreadsheets/d/1hK-q9SLPZxPDDyrhv8PWn11wehqlXzwXzTsjUpOupb0/edit#gid=125397872", "survey")
```

## Read Survey Data

```{r}

# Read data
endline_data <- read.csv("Raw Data/Endline - Main Data Collection/VCF2 RCT Endline Caregivers.csv")

# Fix the Stupid Stata ""-issue and Dates
endline_data[, sapply(endline_data, class) == 'character'][endline_data[, sapply(endline_data, class) == 'character'] == ""] <- NA

# Format dates
endline_data <- endline_data %>% 
  mutate_at(c("SubmissionDate","starttime","endtime"), as_datetime, format = "%d-%m-%Y %H:%M:%S")

endline_data$submission_week <- floor_date(endline_data$SubmissionDate, "weeks", week_start = 1)
endline_data$submission_date_dateonly <- floor_date(endline_data$SubmissionDate, "day")

# Duration and Z scores
endline_data <- endline_data %>% mutate(zscore_duration = (duration - mean(duration))/sd(duration))

# Drop irrelevant variables
#endline_data <- endline_data %>% 
#  select(-c("devicephonenum", "id1", "id2", "id3", "id4", "id5"))

# Retain only responses where the participant consented
tblFun(endline_data$consent)
endline_data <- endline_data %>% filter(consent == 1)
endline_data <- endline_data %>% filter(!is.na(mpesa_incentive_phonenumber))

# Filter for Endline Pilots
endline_data <- endline_data %>% filter(SubmissionDate > as_datetime("2024-06-20 00:00:00"))

# Filter for Main Data Collection
#endline_data <- endline_data %>% filter(SubmissionDate > as_datetime("2023-10-24 00:00:00"))

```

## Clean the randomised questions

```{r}

################### Discernment

# Coalesce the truth discernment columns
for(i in 1:16){
  endline_data[paste0("truth_discernment_",i)] <- do.call(coalesce, endline_data[,paste0("truth_discernment_", i, "_set",1:5)])
}

# Drop the truth discernment columns
old_discernment_var_names <- apply(expand.grid("truth_discernment_",1:16,"_set",1:5), 1, paste0, collapse="")
old_discernment_var_names <- str_remove_all(old_discernment_var_names, " ")
endline_data[old_discernment_var_names] <- NULL

################### Manipulation

# First ensure all the multi-select variable are character variables
multi_select_manip_var_names <- apply(expand.grid("detect_misinfo_",1:12, "_2_set",1:5), 1, paste0, collapse="")
multi_select_manip_var_names <- str_remove_all(multi_select_manip_var_names, " ")
endline_data[,multi_select_manip_var_names] <- data.frame(lapply(endline_data[,multi_select_manip_var_names], as.character))

# Coalesce the manipulation columns
for(i in 1:12){
  
  # Is this manipulative?
  endline_data[paste0("detect_misinfo_",i,"_1")] <- do.call(coalesce, endline_data[,paste0("detect_misinfo_", i, "_1_set",1:5)])
  
  # Which techniques?
  endline_data[paste0("detect_misinfo_",i, "_2")] <- do.call(coalesce, endline_data[,paste0("detect_misinfo_", i, "_2_set",1:5)])
  
  # How confident?
  endline_data[paste0("detect_misinfo_",i, "_3")] <- do.call(coalesce, endline_data[,paste0("detect_misinfo_", i, "_3_set",1:5)])
  
  # Would you share?
  endline_data[paste0("detect_misinfo_",i, "_4")] <- do.call(coalesce, endline_data[,paste0("detect_misinfo_", i, "_4_set",1:5)])
  
  for(multi in c(1:6,88)){
   
    # Technique multiple select variable
    endline_data[paste0("detect_misinfo_",i,"_2_",multi)] <- do.call(coalesce, endline_data[,paste0("detect_misinfo_", i, "_2_set",1:5, "_", multi)])
     
  }
}

# Drop the truth discernment columns
old_manipulation_var_names_1 <- apply(expand.grid("detect_misinfo_",1:12, "_", 1:4, "_set",1:5), 1, paste0, collapse="")
old_manipulation_var_names_1 <- str_remove_all(old_manipulation_var_names_1, " ")
old_manipulation_var_names_2 <- apply(expand.grid("detect_misinfo_",1:12, "_2_set",1:5, "_", c(1:6,88)), 1, paste0, collapse="")
old_manipulation_var_names_2 <- str_remove_all(old_manipulation_var_names_2, " ")
old_manipulation_var_names <- c(old_manipulation_var_names_1, old_manipulation_var_names_2)
endline_data[old_manipulation_var_names] <- NULL

```

## Manually edit data

### Mislabelled Clusters

```{r}

# Prep the function
relabel_clusters <- function(finalids, correct_cluster){
  
  # What is the clusters treatment group 
  correct_cluster_treatment_group <- unique(endline_data$cluster_treatment_group[endline_data$B0_cluster == correct_cluster])
  
  # Adjust clusterID
  endline_data[endline_data$final_ID %in% finalids, ]$B0_cluster <<- correct_cluster
  
  if(length(correct_cluster_treatment_group) > 1){
    print("ERROR")
  }
  
  # Adjust treatment group
  endline_data[endline_data$final_ID %in% finalids, ]$cluster_treatment_group <<- correct_cluster_treatment_group
}


# THIS IS HOW THE CODE WOULD LOOK LIKE
# relabel_clusters(c("67105Dec15104756", "64199Nov1141133"), 97)

```

### Remove Fradulent Data

```{r}

# Fill if needed

```

### Manually adjust FO mistakes

```{r}

# Fill if needed

# Duplicate ID: 13111Nov24111850
endline_data <- endline_data[!(endline_data$id == "13111Nov24111850" & endline_data$deviceid == "ab87c0b51242a8c6"),]

# Wrong ID: 11189Nov21145958 --> 18120Dec7141846
endline_data[endline_data$id == "11189Nov21145958" & endline_data$instanceID == "uuid:809f36bb-e6f8-439e-86ec-da83ceaab0dc",]$id <- "18120Dec7141846"

# Wrong ID: 95175Nov3112751 --> 67108Nov3130717
endline_data[endline_data$id == "95175Nov3112751" & endline_data$instanceID == "uuid:32a0c8e5-ae9a-41e7-96ff-02901f664c23",]$id <- "67108Nov3130717"

# Duplicate ID: 96177Nov18133730
endline_data <- endline_data[!(endline_data$id == "96177Nov18133730" & endline_data$instanceID == "uuid:ca358474-449f-4f30-b0c6-b0e69f7c0502"),]

# Wrong ID: 80157Nov4161400 --> 42158Nov3154605
endline_data[endline_data$id == "80157Nov4161400" & endline_data$instanceID == "uuid:fac1d4b1-7107-4c79-9fea-c2f7e5cbafb4",]$id <- "42158Nov3154605"

# Wrong ID: 12127Dec5100556 --> 83179Nov21114615
endline_data[endline_data$id == "12127Dec5100556" & endline_data$instanceID == "uuid:6d221df7-267f-46e4-919c-f3ef33ccf2f1",]$id <- "83179Nov21114615"

# Wrong ID: 23165Nov2114414 --> 18147Oct30105159
endline_data[endline_data$id == "23165Nov2114414" & endline_data$instanceID == "uuid:8291a689-aa2e-4151-8ce4-838a7722541b",]$id <- "18147Oct30105159"

# Wrong ID: 42154Oct30120549 --> 77120Nov2125652
endline_data[endline_data$id == "42154Oct30120549" & endline_data$instanceID == "uuid:587194c3-ad42-4f41-9515-b92d8e832369",]$id <- "77120Nov2125652"

# Wrong ID: 85198Nov29115551 --> 76187Dec7085123
endline_data[endline_data$id == "85198Nov29115551" & endline_data$instanceID == "uuid:733fc64a-0062-4d4a-9f8a-ebcbe1b6c5ff",]$id <- "76187Dec7085123"

# Wrong ID: 50108Oct27124907 --> 50140Oct31141934
endline_data[endline_data$id == "50108Oct27124907" & endline_data$instanceID == "uuid:8d89180a-242b-4a26-b732-0f95c15d76f2",]$id <- "50140Oct31141934"

# Duplicate ID: 56119Oct24141918
endline_data <- endline_data[!(endline_data$id == "56119Oct24141918" & endline_data$instanceID == "uuid:1c7f2a69-d3b0-49f1-91f9-2a04980cecbf"),]

# Duplicate ID: 57176Oct24111725
endline_data <- endline_data[!(endline_data$id == "57176Oct24111725" & endline_data$instanceID == "uuid:5c796184-bebd-40af-ba09-28bb6767467f"),]

# Wrong Gender: 83158Nov29114440
endline_data[endline_data$id == "83158Nov29114440",]$B2_gender <- 1

# Wrong Gender: 56101Nov13110534
endline_data[endline_data$id == "56101Nov13110534",]$B2_gender <- 1

# Wrong Gender: 29156Nov20170119
endline_data[endline_data$id == "29156Nov20170119",]$B2_gender <- 1

# Wrong Gender: 30105Nov21170133
endline_data[endline_data$id == "30105Nov21170133",]$B2_gender <- 1

# Wrong ID: 32106Nov20144343 --> 99177Nov30093726
endline_data[endline_data$id == "32106Nov20144343" & endline_data$instanceID == "uuid:dfdb6f17-0607-4a3a-a32b-d28cade5703e",]$id <- "99177Nov30093726"

# Wrong Gender: 45194Nov11094858
endline_data[endline_data$id == "45194Nov11094858",]$B2_gender <- 2

# Duplicate ID: 41190Nov27113625
endline_data <- endline_data[!(endline_data$id == "41190Nov27113625" & endline_data$instanceID == "uuid:91557f90-c335-44a4-a4e1-7377ba779d24"),]

# Duplicate ID: 89110Dec14170539 is the same person as 27138Oct27164044
endline_data <- endline_data[!(endline_data$id == "89110Dec14170539" & endline_data$instanceID == "uuid:b9d015d2-14dc-4146-af29-008859c810ce"),]

# Wrong Cluster ID: 88174Oct24111112
endline_data[endline_data$id == "88174Oct24111112",]$B0_cluster <- 201

# Duplicate ID: 77163Nov8135745
endline_data <- endline_data[!(endline_data$id == "77163Nov8135745" & endline_data$instanceID == "uuid:d6ebf57b-04e5-46a9-9788-f7da94705e5c"),]

# Wrong ID: 76112Dec5092040 --> 82190Dec1142951
endline_data[endline_data$id == "76112Dec5092040" & endline_data$instanceID == "uuid:c89b678a-13b7-49d4-9aab-edb19da9f8e2",]$id <- "82190Dec1142951"

# Wrong ID: 49196Oct30123236 --> 106182Oct28124712
endline_data[endline_data$id == "49196Oct30123236" & endline_data$instanceID == "uuid:7a652496-b3f0-4843-b387-f1f882cf4cdd",]$id <- "106182Oct28124712"

#endline_data[endline_data$id == "49196Oct30123236",]$instanceID

```

# ------ Caregivers ------ 

# 1. Base Checks

## 1.1 Did everyone use the correct latest version of the survey?

```{r}

tblFun_II(endline_data$formdef_version, endline_data$submission_date_dateonly) 
# From the 19th June 2024 onwards everyone has to use the version 2406191649
# From the 24th of June 2024, everyone has to use the version 2406241634
# From the 10th of July 2024, everyone has to use the version 2407091146


# Who used the wrong survey?
endline_data %>% 
  group_by(en_name, formdef_version) %>% 
  filter(submission_date_dateonly > as_date("2024-07-05")) %>%
  summarise(n()) %>% 
  arrange(en_name)


# Filter out wrong versions
# endline_data <- endline_data %>% filter(formdef_version >= 123456789)

```

## 1.2 Match with Baseline Responses?

```{r}

# CHECK IF base_end_match_score is more than 1 and look at it.
table(endline_data$base_end_match_score == 0) # SHOULD ONLY BE TRUE

```


## 1.3 Does each completed survey have a unique surveyID?

```{r}

# Should be 0 - anything above 0 indicates there is a surveyID missing
sum(is.na(endline_data$id))

# Should be 0 - anything above 0 means there is a duplicate surveyID
sum(duplicated(endline_data$id))

duplicated_ids <- endline_data$id[duplicated(endline_data$id)]
endline_data[endline_data$id %in% duplicated_ids,] %>% arrange(id) %>% dplyr::select(SubmissionDate, final_ID_baseline, en_name, B1_name)

# Delete buggy duplicate responses (delete the first)
## endline_data <- endline_data[-which(endline_data$final_ID == "15174Oct28132521")[1],] --> IN CASE NEEDED WE CAN EDIT THIS

# Should be 0 - anything above 0 means there is a duplicate surveyID
sum(duplicated(endline_data$id))

```

# CLEANED DATA FROM HERE ON

```{r}
# Export cleaned Data
write.csv(endline_data, "Data Exports/vcf2_rct_endline_caregivers_clean.csv")
```



## 1.4 Are there any programming bugs (e.g., response scale recorded incorrectly, skip logic flaws, all NAs, etc.)?

```{r}

# Is there anyone without children in the prescribed age range?
table(endline_data$B6_childnu_count)
mean(!is.na(endline_data$different_number_children))


# Questions with all NA?
var_logic_all_na <- apply(endline_data, MARGIN = 2, FUN = function(x){sum(is.na(x)) == length(x)})
var_all_na <- names(var_logic_all_na[var_logic_all_na])
var_all_na

# remove clutter
rm(var_logic_all_na, var_all_na)

```

## 1.5 Use of Specify other too much?

```{r}
# Extract specify other vars
specify_other_vars <- colnames(endline_data)[which(str_extract_all(colnames(endline_data), "specify") == "specify")]

# This will be a lot. Check all the specify others variables and examine if you maybe missed some options for answers
# Also subset for the columns that are actually in the survey: Keep in mind, in this phase the survey might still change.--> i.e. this prints all responses given to the "Specify Other" variables
as.list(endline_data[,specify_other_vars[specify_other_vars  %in% colnames(endline_data)]]) %>% lapply(function(x) x[!is.na(x)])

# remove clutter
rm(specify_other_vars)
```

## 1.6 Descriptive Stats

### 1.6.1 Breakdown of Demographics (check for imbalances)

```{r}

# What are the key demographics?
key_demographics <- c("B2_gender", "B3_age", "B4_countylive", "B6a_number", "income_month", "relationship_status", "religion", "christianity_denomination")

# Create temp dataset to label (i.e. change the data)
temp_data <- endline_data[,key_demographics]

# Label the values properly
choice_sets <- str_remove_all(survey_cto_survey[survey_cto_survey$name %in% key_demographics,]$type, "select_one ")
corresponding_vars <- survey_cto_survey[survey_cto_survey$name %in% key_demographics,]$name
choices_key_demographics <- surveycto_choices[surveycto_choices$list_name %in% choice_sets,]

for(i in 1:length(choice_sets)){
  
  if(choice_sets[i] == "integer"){
    next
  }
  
  # Extract choice set for specific demographic
  choices_demographic_x <- choices_key_demographics[choices_key_demographics$list_name == choice_sets[i],]
  
  # Create mapping vector
  map <- unlist(choices_demographic_x$label)
  names(map) <- choices_demographic_x$value
  
  # Re-Label for the corresponding variable
  temp_data[,corresponding_vars[i]] <- unname(map[temp_data[,corresponding_vars[i]]])
  
}


# Change variable names to questions asked
colnames(temp_data)[1:(length(key_demographics)-6)] <- survey_cto_survey$label[match(colnames(temp_data)[1:(length(key_demographics)-6)], survey_cto_survey$name)]

# Compute Summary Stats for the key demographics
lapply(temp_data, tblFun)

# Compute Summary Stats by Treatment Group
lapply(temp_data, tblFun_II, endline_data$cluster_treatment_group)

temp_data_demographics <- temp_data

```

### 1.6.2 Key variables

#### 1.6.2.1 Truth Discernment

```{r}

truth_actuals <- c(truth_discernment_1 = "Girls vaccinated against HPV will not get their menstrual cycles during their liftime.", 
                   truth_discernment_2 = "The HPV vaccine exposes your child to cancer.",
                   truth_discernment_3 =	"If adolescent girls take the vaccine, they will not be able to have children",
                   truth_discernment_4 =	"HPV vaccine is part of a program run by the government to reduce the population.",
                   truth_discernment_5 = "Certain types of HPV can cause genital warts and cervical and penile cancer.",
                   truth_discernment_6 =	"9 women die in Kenya every day due to cervical cancer",
                   truth_discernment_7 = "There exists plenty of scientfic evidence that the HPV vaccine is safe and effective, protecting against cervical cancer.",
                   truth_discernment_8 =	"The HPV vaccine does not cause infertility",
                   truth_discernment_9 = "Wearing masks, maintaining social distance and washing your hands is not effective and does not protect against COVID-19.",
                   truth_discernment_10 = "Secondary education in Kenya lasts for 7 years.",
                   truth_discernment_11 = "The eye scan by Worldcoin is linked to the Illumati.",
                   truth_discernment_12 = "Ovens cause cancer by converting the nutrients of food into dangerous and harmful particles.",
                   truth_discernment_13 = "The leading cause of death in Africa are lung infections, diarrhoeal diseases, HIV/Aids, malaria and tuberculosis.",
                   truth_discernment_14 = "Safaricom is the largest telecommunications provider in Kenya.",
                   truth_discernment_15 = "Nairobi is home to the largest slum in Africa.",
                   truth_discernment_16 = "Kenya is a member of the East African Community.")


# Create temp dataset to label (i.e. change the data)
truth_discernment_vars <- paste0("truth_discernment_",1:16)
temp_data <- endline_data[,truth_discernment_vars]

# Check each variable
truth_discernment_descriptive <- lapply(temp_data, tblFun)
names(truth_discernment_descriptive) <- paste0(names(truth_actuals), ": ", truth_actuals)
truth_discernment_descriptive

# Check by Treatment Group
truth_discernment_descriptive <- lapply(temp_data, tblFun_II, endline_data$cluster_treatment_group)
names(truth_discernment_descriptive) <- paste0(names(truth_actuals), ": ", truth_actuals)
truth_discernment_descriptive

```

#### 1.6.2.2 Source Credibility

```{r}

source_credibility <- c(source_cred_1	= "The Daily Nation", source_cred_2	= "Other Caregivers", source_cred_3	= "Akothee", source_cred_4	= "Your Hairdresser", source_cred_5	= "Doctors", source_cred_6	= "Ministry of Health (MOH)")

# Create temp dataset to label (i.e. change the data)
source_credibility_vars <- paste0("source_cred_",1:6)
temp_data <- endline_data[,source_credibility_vars]

# Check each variable
descriptive <- lapply(temp_data, hist)
names(descriptive) <- paste0(names(source_credibility), ": ", source_credibility)

# Check by Treatment Group
descriptive_by_group <- lapply(temp_data, tblFun_II, endline_data$cluster_treatment_group)
names(descriptive_by_group) <- paste0(names(source_credibility), ": ", source_credibility)
descriptive_by_group
```


#### 1.6.2.3 Manipulation

```{r}

# Manipulativeness
manipulativeness <- c("Manipulative", 
                      "Manipulative", 
                      "Manipulative", 
                      "Manipulative", 
                      "Non-Manipulative", 
                      "Manipulative", 
                      "Non-Manipulative", 
                      "Non-Manipulative", 
                      "Manipulative", 
                      "Non-Manipulative", 
                      "Non-Manipulative", 
                      "Manipulative")
temp_data <- endline_data[paste0("detect_misinfo_",1:12,"_1")]
manipulative_descriptive <- lapply(temp_data, tblFun)
names(manipulative_descriptive) <- paste(names(manipulative_descriptive), manipulativeness)
manipulative_descriptive

# By Treatment Group
manipulative_descriptive <- lapply(temp_data, tblFun_II, endline_data$cluster_treatment_group)
names(manipulative_descriptive) <- paste(names(manipulative_descriptive), manipulativeness)
manipulative_descriptive

# Techniques
techniques <- c("Conspiracy theory (5), Emotional language use (2)", 
  "Emotional language use (2), Conspiracy theory (5)",
  "Fake account (4), Emotional Language use (2), Conspiracy theories (5)", 
  "Trolling (6)",
  "Non-Manipulative",
  "Fake account (4), Emotional Language Use (2), Conspiracy Theory (5), Discrediting Opponents (1)",
  "Non-Manipulative",
  "Non-Manipulative",
  "Intergroup Polarisation (3), Fake Account (4), Emotional Language Use (2)",
  "Non-Manipulative",
  "Non-Manipulative",
  "Discreding Opponents (1)")
temp_data <- endline_data[paste0("detect_misinfo_",1:12,"_2")]
techniques_descriptive <- lapply(temp_data, tblFun)
names(techniques_descriptive) <- paste(names(techniques_descriptive), techniques)
techniques_descriptive

```

### 1.6.3 Balance Check

```{r}

# Checking Balance using Summary Statistics
check_balance <- function(data, treatment_var, vars){
  for (var in vars){
    print(paste("Checking balance for:", var))
    
    data[data == 77] <- NA
    data[data == 88] <- NA
    data[data == 99] <- NA
    
    # Check variable type
    if (is.numeric(data[[var]])){
      # T-test
      anova_result <- TukeyHSD(aov(data[[var]] ~ data[[treatment_var]]))
      print(anova_result[1])
    } else if (is.factor(data[[var]]) || is.character(data[[var]])) {
      # Chi-squared test for categorical variables
      contingency_table <- table(data[[treatment_var]], data[[var]])
      chi_sq_test <- chisq.test(contingency_table)
      print(paste("Chi-squared test p-value:", chi_sq_test$p.value))
    }
  }
}

# Specify variables to check
demographic_variables_to_check <- key_demographics
manipulation_vars_to_check <- paste0("detect_misinfo_",1:12,"_1")
truth_discernment_var_to_check <- paste0("truth_discernment_",1:16)

# Using the function
check_balance(endline_data, 'cluster_treatment_group', demographic_variables_to_check)
check_balance(endline_data, 'cluster_treatment_group', manipulation_vars_to_check)
check_balance(endline_data, 'cluster_treatment_group', truth_discernment_var_to_check)

```


## 1.7 Randomisation Check

```{r}

ggplot(endline_data) + geom_bar(aes(x = cluster_treatment_group), fill = "#0033A1")

```

## 1.8 High rates of missing data (e.g., Don't Know, Refuse to Respond) for key variables?

```{r}

# Compute share of Don't Knows per variables
head(sort(apply(endline_data, FUN = function(x) mean(x %in% 88), MARGIN = 2), decreasing = T), 10)

# Compute share of Refused to Answer 
head(sort(apply(endline_data, FUN = function(x) mean(x %in% 99), MARGIN = 2), decreasing = T), 10)

```

## 1.9 ICC-check and Power-check

```{r}

#icc <- clus.rho(popchar = endline_data$truth_discernment_1, cluster = endline_data$B0_cluster)
(icc_1 <- clus.rho(popchar = endline_data$detect_misinfo_1_1[endline_data$detect_misinfo_1_1 != 88], cluster = endline_data$B0_cluster[endline_data$detect_misinfo_1_1 != 88]))

(icc_2 <- clus.rho(popchar = endline_data$truth_discernment_1[endline_data$truth_discernment_1 != 88], cluster = endline_data$B0_cluster[endline_data$truth_discernment_1 != 88]))

(icc_3 <- clus.rho(popchar = endline_data$vax_beh_int_1_1[endline_data$vax_beh_int_1_1 != 88], cluster = endline_data$B0_cluster[endline_data$vax_beh_int_1_1 != 88]))

# Predicted
wp.crt2arm(n = 60/2, J = 108/2, icc = ifelse(icc_1$icc[1] < 0,0,icc_1$icc[1]), alpha = 0.05, power = 0.8)

# Actual
temp_df <- endline_data %>% 
  group_by(B0_cluster) %>% 
  summarise(n = n())

temp_df <- temp_df %>% 
  filter(n > 30)
sd(temp_df$n)/mean(temp_df$n)

temp_df <- endline_data %>% 
  group_by(B0_cluster) %>% 
  summarise(n = n()) %>% 
  mutate(mean_n = mean(n))

wp.crt2arm(n = round(mean(temp_df$n)/2),
           J = round(length(unique(temp_df$B0_cluster))/2),
           icc = ifelse(icc_1$icc[1] < 0,0,icc_1$icc[1]), 
           alpha = 0.05, 
           power = 0.8)

wp.crt2arm(n = round(mean(temp_df$n)/2),
           J = round(length(unique(temp_df$B0_cluster))/2),
           icc = ifelse(icc_2$icc[1] < 0,0,icc_2$icc[1]), 
           alpha = 0.05, 
           power = 0.8)

wp.crt2arm(n = round(mean(temp_df$n)/2),
           J = round(length(unique(temp_df$B0_cluster))/2),
           icc = ifelse(icc_3$icc[1] < 0,0,icc_3$icc[1]), 
           alpha = 0.05, 
           power = 0.8)

```

 
# 2. Duplicates & Fraud

## 2.1 Are we confident that each completed survey corresponds to a unique participant? If so, are all survey ID and phone number pairings unique?

```{r}

# Exact duplicate rows? SHOULD BE TRUE
sum(duplicated(endline_data)) == 0

# Duplicate (looking at Name, Phone Number)
# Name
duplicate_names <- endline_data$B1_name[duplicated(tolower(endline_data[,c("B1_name")]))]
temp_df <- endline_data[endline_data$B1_name %in% duplicate_names,c("id", "starttime", "en_name", "B1_name", "B0_cluster", "county_name")] %>% 
  arrange(B1_name, en_name)
temp_df

```

## 2.2 Arbitrary responses?

```{r}

# Separate groups of questions answered by scales
manipulativeness_assessment <- paste0("detect_misinfo_", 1:12,"_1")
truth_discernment <- c("truth_discernment_1","truth_discernment_2","truth_discernment_3","truth_discernment_4","truth_discernment_5","truth_discernment_6","truth_discernment_7","truth_discernment_8","truth_discernment_9","truth_discernment_10","truth_discernment_11","truth_discernment_12","truth_discernment_13","truth_discernment_14","truth_discernment_15","truth_discernment_16")
source_cred <- c("source_cred_1", "source_cred_2", "source_cred_3", "source_cred_4", "source_cred_5", "source_cred_6")
vax_hes <- c("vax_hes_2", "vax_hes_3", "vax_hes_4", "vax_hes_5", "vax_hes_6", "vax_hes_7", "vax_hes_8", "vax_hes_9")
vax_beh_int <- c("vax_beh_int_2", "vax_beh_int_3", "vax_beh_int_4", "vax_beh_int_6")

# Combine into list of variable groups
q_groups <- list(manipulativeness_assessment = manipulativeness_assessment,
                 truth_discernment = truth_discernment,
                 source_cred = source_cred, 
                 vax_hes = vax_hes, 
                 vax_beh_int = vax_beh_int)

######################## Share of respondents per question group that said the same thing for each question

q_groups_copy <- q_groups
for(i in 1:length(q_groups_copy)){
  q_groups_copy[[i]] <- round(mean(apply(endline_data[,q_groups_copy[[i]]], MARGIN = 1, FUN = function(x) length(unique(x)) == 1)), digits = 3)
}

likert_understanding_df <- data.frame(share_only_1_response = t(data.frame(q_groups_copy))) %>% arrange(desc(share_only_1_response)) 
likert_understanding_df$question_group <- rownames(likert_understanding_df)
likert_understanding_df$question_group <- factor(likert_understanding_df$question_group, levels = likert_understanding_df$question_group[order(likert_understanding_df$share_only_1_response, decreasing = TRUE)])

# Print output
likert_understanding_df %>% 
  ggplot() + 
  geom_bar(aes(x = share_only_1_response, y = question_group), stat = "identity", fill = "#0033A1") +
  geom_label(aes(x = share_only_1_response, y = question_group, label = paste0(round(share_only_1_response*100),"%")), nudge_x = 0.05) +
  scale_x_continuous(limits = c(0,1), labels = c("0%", "25%", "50%", "75%", "100%")) +
  xlab("Share of Respondents who only\n gave a single response\n to all questions") + ylab("Question Group") +
  theme_minimal()


############################ Per respondent show the share of groups where the respondent said the same thing for every question

# Create dataframe that capture the amount of distinct responses per group per participant
distinct_q_groups <- lapply(q_groups, function(i) apply(endline_data[,i], MARGIN = 1, FUN = n_distinct, na.rm = T))
distinct_q_groups_df <- data.frame(do.call(cbind, distinct_q_groups))

#Show the share of groups per respondent where the same response was given for each question in a question group
share_groups_by_respondent_only_1_response <- apply(distinct_q_groups_df, MARGIN = 1, FUN = function(i) mean(i == 1))
hist(share_groups_by_respondent_only_1_response, main = "Share of Groups per Respondents\nwithout variation", xlab = "Share of Groups", col = "#0033a1")

# Show the groups in which this took place
which_groups_by_respondent_only_1_response <- apply(distinct_q_groups_df, MARGIN = 1, FUN = function(i) colnames(distinct_q_groups_df)[i == 1])

# Print a dataframe showing each respondent (name, email), the number of groups (and share they answered with only one response) and the groups as a combined string
arbitrary_responses_df <- data.frame(B1_name = endline_data$B1_name, 
                                     final_ID = endline_data$final_ID,
                                     endtime = endline_data$endtime,
                                     en_name = endline_data$en_name, 
                                     B0_cluster = endline_data$B0_cluster,
                                     county_name = endline_data$county_name,
                                     groups_single_resp = sapply(which_groups_by_respondent_only_1_response, paste, collapse = ", "), 
                                     number_groups_single_resp = sapply(which_groups_by_respondent_only_1_response, length), 
                                     share_groups_single_resp = share_groups_by_respondent_only_1_response) %>%
  arrange(desc(share_groups_single_resp))

arbitrary_responses_df$groups_single_resp[arbitrary_responses_df$groups_single_resp == ""] <- "No_Single_Resp_Group"

# Who messed up?
temp_df <- arbitrary_responses_df %>% 
  arrange(en_name, desc(share_groups_single_resp))
temp_df

# remove clutter
#rm(distinct_q_groups, distinct_q_groups_df, which_groups_by_respondent_only_1_response, share_groups_by_respondent_only_1_response, likert_understanding_df, q_groups, q_groups_copy, i)

```


## 2.4 Is there evidence of fake data? (how long did surveys take)

```{r}

# Show the time histogram to get a feel for how long people need
hist(endline_data$duration/60) # In Minutes

# Someone took Xh, let's exclude him to see the general pattern better
#hist(endline_data$duration[endline_data$duration/60 < X]/60, main = "Duration (filtered by <Xh)")
endline_data %>% group_by(en_name) %>% summarise(duration = mean(duration/60)) %>% arrange(duration)


# Investigate the histogram of z scores.
hist(endline_data$zscore_duration)
# Show the observations that are more than 2 standard deviations below the mean duration (very short responses, indicating they just ticked through)
time_outliers_df <- endline_data[endline_data$zscore_duration < -2,]

# remove clutter
#rm(time_outliers_df)
```

## 2.5 Locations

```{r}

############################ BASELINE

# Read in and convert baseline data to shp file
baseline_data_reduced_clean <- read.csv("Data Exports/vcf2_rct_baseline_caregivers_clean_reduced.csv")

survey_locations_baseline <- SpatialPointsDataFrame(coords = cbind(baseline_data_reduced_clean$geopoint_recruitment.Longitude, baseline_data_reduced_clean$geopoint_recruitment.Latitude), data = baseline_data_reduced_clean[,c("B0_cluster", "en_name", "county_name", "geopoint_recruitment.Accuracy", "final_ID", "endtime", "submission_date_dateonly", "cluster_treatment_group")], proj4string = CRS("EPSG:4326"))

# Convert data format
survey_locations_baseline <- st_as_sf(survey_locations_baseline)

# Write to look at in QGIS
st_write(survey_locations_baseline, dsn = "Data Exports/survey_locations_baseline.shp", append = F)

########################### ENDLINE

# Location of SurveyIDs
survey_locations <- SpatialPointsDataFrame(coords = cbind(endline_data$geopoint_endline.Longitude, endline_data$geopoint_endline.Latitude), data = endline_data[,c("B0_cluster", "en_name", "county_name", "geopoint_endline.Accuracy", "id", "endtime", "submission_date_dateonly", "cluster_treatment_group")], proj4string = CRS("EPSG:4326"))

# Convert data format
survey_locations <- st_as_sf(survey_locations)

# Read locations of clusters and find survey locations that were outside of the clusters
vcf_rct_clusters <- st_read("Sampling/clusters/randomly_selected_clusters_buffered_allocated_to_groups_v6.shp")

# Compute minimum distance between polygon and points to see if either points are within polygons or which ones they are closest to.
point_in_polygon <- st_distance(vcf_rct_clusters, survey_locations)
cluster_min_dist_in_meters <- apply(point_in_polygon, MARGIN = 2, FUN = min)

# Find the closest cluster or the cluster in which the response was logged
point_in_cluster_min_idx <- apply(point_in_polygon, MARGIN = 2, FUN = which.min)
in_which_close_to_which <- vcf_rct_clusters$cluster_id[point_in_cluster_min_idx]

# Add to survey_locations object
survey_locations <- survey_locations %>% 
  mutate(cluster_min_dist_in_meters = cluster_min_dist_in_meters) %>% 
  add_column(in_which_close_to_which = in_which_close_to_which, .after = 1) %>% 
  mutate(within_cluster = ifelse(cluster_min_dist_in_meters == 0, T, F))



############# WERE SURVEYS CONDUCTED CLOSE TO THE PREVIOUS SURVEYS

# Convert and merge dfs
survey_locations_endline <- as.data.frame(survey_locations)
survey_locations_baseline <- as.data.frame(survey_locations_baseline)
merged_survey_locations_df <- merge(survey_locations_baseline, survey_locations_endline, by.x = "final_ID", by.y = "id", suffixes = c("_1", "_2"))

# Function to compute Euclidean distance between two points
compute_distance <- function(geom1, geom2) {
  st_distance(geom1, geom2)
}

# Apply the function to each row
merged_survey_locations_df <- merged_survey_locations_df %>%
  rowwise() %>%
  mutate(distance = compute_distance(geometry_1, geometry_2)) %>% 
  mutate(distance_meters = as.numeric(distance))

# Merge in the distances to export
match_idx <- match(survey_locations$id, merged_survey_locations_df$final_ID)
survey_locations$distance <- as.numeric(merged_survey_locations_df$distance)[match_idx]
survey_locations$distance <- round(survey_locations$distance, 1)

# Write to look at in QGIS
st_write(survey_locations, dsn = "Data Exports/survey_locations_endline.shp", append = F)


########### To actually look at




# Which survey submissions were not in the cluster
survey_locations %>% 
  filter(within_cluster == FALSE) %>% 
  #filter(submission_date_dateonly >= "2023-10-30") %>% 
  group_by(en_name) %>% 
  summarise(sum_outside = sum(!within_cluster), mean_distance = mean(cluster_min_dist_in_meters), mean_accuracy_gps = mean(geopoint_endline.Accuracy)) %>% # Might this be due to the poor accuracy of the GPS
  arrange(desc(sum_outside))

# Which clusters were mislabelled 
survey_locations %>% 
  filter(B0_cluster != in_which_close_to_which) %>% 
  group_by(en_name) %>% 
  mutate(n = n(), .before = "B0_cluster")

```

## 2.6 Audio Audit / Backcheck - Pipeline

```{r}

############### Share of Audio Consent Rejections per FO
endline_data %>%
  filter(submission_date_dateonly > as_date("2024-07-01")) %>%
  group_by(en_name) %>% 
  summarise(mean_audio_rejection = mean(audio_consent == 0, na.rm = T)) %>% 
  arrange(desc(mean_audio_rejection))


################### which responses don't have audio?
endline_data %>% 
  filter(submission_date_dateonly > as_date("2024-07-01")) %>%
  filter(audio_consent == 1) %>% 
  filter(is.na(audio)) %>% 
  group_by(en_name, county_name, submission_date_dateonly) %>% 
  summarise(number_no_audio = n()) %>% 
  arrange(county_name, desc(number_no_audio))


```


+++++++++++++ YOU HAVE TO ADD THE MINI-TREATMENT GROUP INDICATOR!!!! ++++++++++++++++++


### 2.6.1 Regular Audio Audits

```{r}

#################### Audit 25 additional surveys per week

# Find out the last date that was uploaded and then filter for dates larger than that.
regular_audits_so_far <- read_sheet(ss = "https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
           sheet = "Regular_Audio_Audits")

# Create inidcator if people were in the control mini treatment group
endline_data$mini_treatment_check <- endline_data$random_number_for_mini_treatment > 0.5 & endline_data$cluster_treatment_group == "Control"

set.seed(42)
temp_df2 <- endline_data %>%
  filter(!is.na(audio)) %>% 
  filter(audio_consent == 1) %>%
  #filter(submission_date_dateonly > max(as_date(regular_audits_so_far$SubmissionDate) + day(1))) %>% 
  group_by(submission_date_dateonly) %>% 
  mutate(submission_date_dateonly = as.character(submission_date_dateonly)) %>%
  mutate(duration_mins = round(duration/60)) %>% 
  slice_sample(n = 12) %>% 
  dplyr::select(id, submission_date_dateonly, duration_mins, en_name, county_name_fo_location, B0_cluster, B1_name, audio, mini_treatment_check) %>% 
  mutate(mini_treatment_check = as.character(mini_treatment_check))

range_write(temp_df2, 
            ss = "https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
            sheet = "Regular_Audio_Audits",
            range = paste0("A",nrow(regular_audits_so_far)+2),
            reformat = F,
            col_names = F)

```

### 2.6.2 Manual Audio Audits

```{r}

############ Add manual Audio Audits


# Add the IDs you want to audit
jones_kipsang_consent <- c("39131Oct27113059", "28157Nov1104012")
ogada_too_short <- c("23139Nov27104052")
macy_miriam_too_short <- c("43162Nov1124838")
oscar_musumba_a_few_shorties <- c("82194Nov11153544", "95165Nov10120338")
macy_miriam_quick_succession <- c("87151Nov11184300", "106122Nov6130151")
sahara_mohammed_arbitrary_responses <- c("96135Oct24111724", "79126Nov24100010")
winnie_wafula_extra <- c("40192Nov27150803", "95169Dec11124057")

# Add more audits for those that were dismissed or temporarily suspended to establish consistency over time of their behaviour or identify when it started/stopped to delete data in an informed way

# Read overview data of audio audits of frauds
temp <- read_sheet("https://docs.google.com/spreadsheets/d/17sU9kZRTGZQYdCAqtMqpXN6dd7NU-KQ4MHW5tOQDbfM/edit#gid=0",
           sheet = "FOs suspected of fraud")

# For which dates did we do audios
all_detected_dates <- str_extract_all(temp$`When did they do surveys poorly`, "[0-9]+[thsrd]{2} [A-Za-z]{3} [0-9]{4}")
all_detected_dates_clean <- lapply(all_detected_dates, str_remove_all, "st|nd|rd|th")
all_detected_dates_dateobj <- lapply(all_detected_dates_clean, as.Date, format = "%d %b %Y")
bad_data_date_ranges <- lapply(all_detected_dates_dateobj, range)
bad_data_date_ranges_df <- data.frame(cbind(temp$`FO name`, t(data.frame(bad_data_date_ranges))), row.names = NULL)
colnames(bad_data_date_ranges_df) <- c("en_name", "first_bad_detected", "last_bad_detected")

# Get more data to cover all weeks of endline data collection
tmp_list <- lapply(bad_data_date_ranges_df$en_name, function(i) range(endline_data$SubmissionDate[endline_data$en_name == i]))
bad_data_date_ranges_df <- cbind(bad_data_date_ranges_df, t(data.frame(tmp_list)), row.names = NULL)
colnames(bad_data_date_ranges_df) <- c("en_name", "first_bad_detected", "last_bad_detected", "first_survey", "last_survey")

uncover_unexplored_dates <- function(fo_name){
  fo_date_range_all <- endline_data$SubmissionDate[endline_data$en_name == fo_name]
  fo_date_range_bad_detected <- as_date(unlist(bad_data_date_ranges_df[bad_data_date_ranges_df$en_name == fo_name,c("first_bad_detected", "last_bad_detected")]))
  fo_date_range_unexplored <- fo_date_range_all[fo_date_range_all < fo_date_range_bad_detected[1] | fo_date_range_all > fo_date_range_bad_detected[2]]
  
  return(fo_date_range_unexplored)
}

x <- lapply(bad_data_date_ranges_df$en_name, uncover_unexplored_dates)
names(x) <- bad_data_date_ranges_df$en_name

sample_more_audits <- function(fo_name, n){
  
  fo_date_range_bad_detected <- as_date(unlist(bad_data_date_ranges_df[bad_data_date_ranges_df$en_name == fo_name,c("first_bad_detected", "last_bad_detected")]))

  temp <- endline_data %>% 
    filter(en_name == fo_name) %>% 
    mutate(submission_date_dateonly = as_date(submission_date_dateonly)) %>% 
    filter(submission_date_dateonly < fo_date_range_bad_detected[1] | submission_date_dateonly > fo_date_range_bad_detected[2]) %>% 
    filter(!is.na(audio)) %>% 
    dplyr::select(final_ID, submission_date_dateonly) %>% 
    sample_n(n)
  
  return(temp$final_ID)
}

# Sample more for the cheaters
set.seed(24)


# COMPILE FINAL VECTOR WITH ALL FINAL_IDs 
manual_audio_audits <- c(jones_kipsang_consent, ogada_too_short, macy_miriam_too_short, oscar_musumba_a_few_shorties, macy_miriam_quick_succession, sahara_mohammed_arbitrary_responses, winnie_wafula_extra)

manual_audio_audits_unique <- unique(manual_audio_audits)

temp_df2 <- endline_data %>% 
  filter(id %in% manual_audio_audits) %>%
  mutate(submission_date_dateonly = as.character(submission_date_dateonly)) %>%
  mutate(duration_mins = round(duration/60)) %>% 
  dplyr::select(id, submission_date_dateonly, duration_mins, en_name, county_name_fo_location, B0_cluster, B1_name, audio, mini_treatment_check)

# Order properly
temp_df2 <- temp_df2[match(manual_audio_audits, temp_df2$id),] 
temp_df2 <- temp_df2[!is.na(temp_df2$id),]

range_write(data = temp_df2, 
            ss = "https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
              sheet = "Manual_Audio_Audits",
              range = "A2",
              reformat = F,
              col_names = F)

```

### 2.6.3 Backchecks

```{r}
############# Add manual backchecks

jones_kipsang_audio_consent <- c("36180Dec6101545", "104104Oct27123507")
peter_masoni_too_short <- c("85114Nov9093055")
oscar_musumba_too_short <- c("65116Nov15155558", "65139Nov11154439", "42104Dec12105711")
macy_miriam_quick_succession <- c("33179Nov7165411")
oscar_musumba_too_short <- c("54185Nov10123526")

manual_backchecks <- unique(c(jones_kipsang_audio_consent, peter_masoni_too_short, oscar_musumba_too_short))

# Merge in Phone numbers from Baseline
id_df <- data.frame(final_ID = manual_backchecks)

# Perform a left join to get phone numbers in the order of the IDs in the vector
extracted_phonenumber_df <- id_df %>%
  left_join(baseline_data_reduced_clean, by = "final_ID") %>% 
  dplyr::select(final_ID, B7_phonenumber)

endline_data %>% 
  filter(id %in% manual_backchecks) %>%
  mutate(submission_date_dateonly = as.character(submission_date_dateonly)) %>%
  mutate(duration_mins = round(duration/60)) %>% 
  dplyr::select(id, submission_date_dateonly, duration_mins, en_name, county_name_fo_location, B0_cluster, B1_name, audio) %>%
  left_join(extracted_phonenumber_df, by = join_by(id == final_ID)) %>% 
  dplyr::select(id, submission_date_dateonly, duration_mins, en_name, county_name_fo_location, B0_cluster, B1_name, B7_phonenumber, audio) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
              sheet = "Backchecks",
              range = "A2",
              reformat = F,
              col_names = F)

```

### 2.6.4 Audit Outcome Visualisation

```{r}

# Read in all audits/backchecks
regular_audio_audits <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
           sheet = "Regular_Audio_Audits")

manual_audio_audits <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
           sheet = "Manual_Audio_Audits") %>% 
  filter(!is.na(audio))

backchecks <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=1617301493",
           sheet = "Backchecks")


# Bind into 1 data.frame
all_checks_outcomes <- data.frame(rbind(cbind(regular_audio_audits$en_name, regular_audio_audits$SubmissionDate,regular_audio_audits$`Final Outcome (Box Ticking)`), cbind(manual_audio_audits$en_name, manual_audio_audits$SubmissionDate, manual_audio_audits$`Final Outcome (Box Ticking)`), cbind(backchecks$en_name, backchecks$SubmissionDate, backchecks$`Final Outcome (Box Ticking)`)))
colnames(all_checks_outcomes) <- c("en_name", "Date", "Outcome")

# Convert Date to a factor for stacking in ggplot
all_checks_outcomes$Date <- as.factor(all_checks_outcomes$Date)


# Create the ggplot stacked bar chart
ggplot(all_checks_outcomes) +
  geom_bar(aes(x = en_name, fill = Outcome), stat = "count", position = "stack") +
  facet_grid(~ Date) +
  labs(y = "Entity Name", x = "Count", fill = "Outcome") +
  theme_minimal()


ggplot(all_checks_outcomes[order(all_checks_outcomes$Date,decreasing=T),]) +
  geom_bar(aes(x = Date, y = en_name, fill = Outcome), stat = "identity", position = "stack") +
  scale_fill_manual(values = c("Poor Quality (not fraud)" = "yellow", "All good" = "green", "Fraud" = "red")) +
  theme_minimal()

plot <- all_checks_outcomes %>% 
  arrange(desc(Date)) %>% 
  mutate(Date = str_remove_all(as.character(Date), "-"), Date = as.numeric(Date), Date = as.factor(Date)) %>%
  ggplot() +
  geom_bar(aes(x = en_name, y = Date, fill = Outcome), stat = "identity") +
  scale_fill_manual(values = c("Poor Quality (not fraud)" = "yellow", "All good" = "green", "Fraud" = "red")) +
  theme_minimal() +
  coord_flip()
ggplotly(plot)

plot <- ggplot(all_checks_outcomes) +
  geom_point(aes(x = Date, y = en_name, color = Outcome)) +
  scale_color_manual(values = c("Poor Quality (not fraud)" = "yellow", "All good" = "green", "Fraud" = "red")) +
  theme_minimal() +
  theme(panel.grid = element_blank())
ggplotly(plot)

```

# 3. Tracking

## 3.2 Tracking Progress

```{r}

# How many surveys are completed overall
endline_data %>% summarise(n_overall = n())

# By County
endline_data %>% 
  group_by(county_name) %>% 
  summarise(n_by_county = n()) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1BH4vi0MZoFPH6FzZxaDE3NkeRfCAaWWkgsS4EhIMzZk/edit#gid=0",
              sheet = "Overall Progress",
              range = "A2",
              col_names = F,
              reformat = F)

# By cluster
endline_data %>% 
  group_by(B0_cluster) %>% 
  summarise(n_by_cluster = n()) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1BH4vi0MZoFPH6FzZxaDE3NkeRfCAaWWkgsS4EhIMzZk/edit#gid=0",
              sheet = "Overall Progress",
              range = "D2",
              col_names = F,
              reformat = F)

# By FO
endline_data %>% 
  group_by(en_name) %>% 
  summarise(n_by_FO = n()) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1BH4vi0MZoFPH6FzZxaDE3NkeRfCAaWWkgsS4EhIMzZk/edit#gid=0",
              sheet = "Overall Progress",
              range = "G2",
              col_names = F,
              reformat = F)


# By County By FO
endline_data %>% 
  group_by(county_name, en_name) %>% 
  summarise(n_byCounty_byFO = n()) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1BH4vi0MZoFPH6FzZxaDE3NkeRfCAaWWkgsS4EhIMzZk/edit#gid=0",
              sheet = "Overall Progress",
              range = "J2",
              col_names = F,
              reformat = F)

```

## 3.3 Forecast Progress

```{r eval=FALSE, include=FALSE}

temp_data <- endline_data %>% 
  group_by(submission_date_dateonly) %>% 
  summarise(n_by_day = n())

# Calculate the cumulative sum
temp_data$cum_sum <- cumsum(temp_data$n_by_day)

# Convert date_day to Date class and set it as a ts (time series) object
ts_data <- ts(temp_data$cum_sum, start=c(year(min(temp_data$submission_date_dateonly)), yday(min(temp_data$submission_date_dateonly))), frequency=365)

# Use auto.arima() to automatically fit the best ARIMA model
fit <- auto.arima(ts_data)

# Forecast up to a specified date
days_to_forecast <- as.numeric(as.POSIXct("2024-07-23", tz = "UCT") - max(temp_data$submission_date_dateonly))
forecasted <- forecast(fit, h=days_to_forecast)

# Plot the forecast
plot(forecasted)

# Plot again with ggplot
forecasted_df <- data.frame(forecasted)
forecasted_df$date <- seq(as_date(max(temp_data$submission_date_dateonly)+1), by = "days", length.out = days_to_forecast)

plot <- forecasted_df %>% 
  ggplot() + 
  geom_line(aes(y = Point.Forecast, x = date)) +
  theme_minimal()
ggplotly(plot)

```

## 3.4 Optimising Progress

```{r}

# Cumulative Progress Across Clusters, Projections on Completion (Are clusters moving together?)
endline_data %>% 
  group_by(B0_cluster) %>% 
  summarise(n = n()) %>% 
  arrange(B0_cluster)

```

```{r}

############################################### COMPUTE AVERAGE DAILY OUTPUT BY CLUSTER

# 1. Average output per day, per cluster, per FO (excluding 0 output days, adjusting for FOs worked in cluster)
average_output <- endline_data %>%
  group_by(B0_cluster, en_name, submission_date_dateonly) %>%
  summarise(daily_responses = n(), .groups = "drop_last") %>%
  group_by(B0_cluster, en_name) %>%
  summarise(average_responses_by_cluster_by_fo = mean(daily_responses[daily_responses > 0]), .groups = "drop_last") %>% 
  summarise(average_responses_by_cluster_by_fo = mean(average_responses_by_cluster_by_fo))

# Assign a baseline productivity to clusters with no data
baseline_productivity <- mean(average_output$average_responses_by_cluster_by_fo, na.rm = T)-1
average_output$average_responses_by_cluster_by_fo[average_output$average_responses_by_cluster_by_fo < 1.00001] <- baseline_productivity

# Add all clusters currently in the game where no data has been collected yet + add baseline productivity
current_clusters <- surveycto_choices[surveycto_choices$list_name == "cluster_id" & !is.na(surveycto_choices$list_name),]$value
#new_clusters <- data.frame(B0_cluster = current_clusters[!(current_clusters %in% average_output$B0_cluster)], average_responses_by_cluster_by_fo = baseline_productivity)
#average_output <- rbind(average_output, new_clusters)

############################################### OPTIMISE FO ALLOCATION AND DIAGNOSE PREDICTED PROGRESS BY 19TH July 2024

# Compute targets per cluster (which is the baseline cluster sizes)
baseline_data <- read.csv("Data Exports/vcf2_rct_baseline_caregivers_clean_reduced.csv")[,-1] 
vcf_baseline_cluster_sizes <- baseline_data %>% 
  dplyr::select(B0_cluster, cluster_treatment_group) %>% 
  group_by(B0_cluster) %>% 
  summarise(target_n = n(), cluster_treatment_group = unique(cluster_treatment_group))
  
# Merge with previous dataframe
average_output_and_targets <- merge(average_output, vcf_baseline_cluster_sizes, by.x = "B0_cluster", by.y = "B0_cluster")

# Compute current sample size per cluster
temp_data <- endline_data %>% group_by(B0_cluster) %>% summarise(n = n()) %>% arrange(B0_cluster)
average_output_and_currents_and_targets <- merge(average_output_and_targets, temp_data, by = "B0_cluster", all.x = T)
average_output_and_currents_and_targets$n[is.na(average_output_and_currents_and_targets$n)] <- 0

# Days left until 19th July 2024
days_left <- as.integer(as.Date("2024-07-19") - max(as.Date(endline_data$submission_date_dateonly)))

# Given average daily progress, and days left how many FOs need to work in which clusters to equalise sample size (assume we have 33 FOs per day.... and on average 1 per day is sick, + 1 dropped out)
fo_days <- days_left*33

# Initialise with uniform distribution of FO days
average_output_and_currents_and_targets$outcome_n <- average_output_and_currents_and_targets$n + ((fo_days/nrow(average_output_and_currents_and_targets))*average_output_and_currents_and_targets$average_responses_by_cluster)
average_output_and_currents_and_targets$fo_days_deployed <- fo_days/nrow(average_output_and_currents_and_targets)

# This function amplifies the re-allocaction to be not linear but exponential, i.e. cluster most in need don't get proportionately more FO days but exponentially more
amplify_and_normalize <- function(vec, power = 1) {
    # Apply a power transformation to each element
    amplified_vec <- vec ^ power

    # Normalize by dividing each element by the sum of the transformed vector
    normalized_vec <- amplified_vec / sum(amplified_vec)

    return(normalized_vec)
}

# This function is the actual optimiser that takes from surplus clusters and gives to lagging clusters
optimize_towards_zero <- function(vec, limit_vec) {
    # Calculate the total amount available for redistribution (sum of the minimum of positive values and their limits)
    total_redistribution <- sum(pmin(vec[vec > 0], limit_vec[vec > 0]))
    
    # Subtract from positive values
    adjusted_vec <- vec
    adjusted_vec[vec > 0] <- vec[vec > 0] - pmin(vec[vec > 0], limit_vec[vec > 0])
    
    # Add to the negative values by share of negativity
    
    share_temp <- amplify_and_normalize(adjusted_vec[adjusted_vec < 0]/sum(adjusted_vec[adjusted_vec < 0]), power = 2)
    adjusted_vec[adjusted_vec < 0] <- adjusted_vec[adjusted_vec < 0] + (total_redistribution*share_temp)

    # Calculate the amount of change for each element
    change_vector <- adjusted_vec - vec

    return(list(original_vector = vec,
                adjusted_vector = adjusted_vec,
                change_vector = change_vector))
}

# This function iterates this optimisation process
iterative_optimizer <- function(data_input, initial_limit_vector, productivity_vector, max_iterations = 100, tolerance = 1e-6) {
  
    diff_outcome_target <- data_input$outcome_n - data_input$target_n
    fo_days_over_under <- diff_outcome_target/productivity_vector

    current_vector <- fo_days_over_under
    limit_vector <- initial_limit_vector
    outcome_n <- initial_limit_vector * productivity_vector
    prev_outcome_n <- outcome_n
    variation <- sd(outcome_n)

    for (i in 1:max_iterations) {
      
        diff_outcome_target <- data_input$outcome_n - data_input$target_n
        fo_days_over_under <- diff_outcome_target/productivity_vector
        
        result <- optimize_towards_zero(current_vector, limit_vector)
        
        # Update limit and outcome vectors
        new_limit_vector <- limit_vector + result$change_vector
        new_outcome_n <- data_input$n + (new_limit_vector * productivity_vector)
        
       
        
        # Check for convergence
        new_variation <- sd(new_outcome_n)
        if (abs(new_variation - variation) < tolerance) {
            break
        }
        
        # Update for next iteration
        current_vector <- result$adjusted_vector
        limit_vector <- new_limit_vector
        outcome_n <- new_outcome_n
        variation <- new_variation # Update variation
        
    }

    return(list(final_outcome_n = new_outcome_n, 
                final_fo_days_deployed = new_limit_vector, 
                final_fo_days_over_under = result$adjusted_vector,
                iterations = i))
}


# Execute the optimisation
temp <- iterative_optimizer(data_input = average_output_and_currents_and_targets,
                            initial_limit_vector = average_output_and_currents_and_targets$fo_days_deployed,
                            productivity_vector = average_output_and_currents_and_targets$average_responses_by_cluster, 
                            max_iterations = 100, 
                            tolerance = 1e-180)

# Add variables to overview data.frame to communicate to SFOs
average_output_and_currents_and_targets$optimised_fo_days_deployed <- temp$final_fo_days_deployed
average_output_and_currents_and_targets$optimised_outcome_n <- temp$final_outcome_n
average_output_and_currents_and_targets$optimised_final_fo_days_over_under <- temp$final_fo_days_over_under
average_output_and_currents_and_targets$predicted_percentage_target_completion <- average_output_and_currents_and_targets$optimised_outcome_n/average_output_and_currents_and_targets$target_n

# Export to Googlesheets
average_output_and_currents_and_targets %>% 
  dplyr::select(B0_cluster, average_responses_by_cluster_by_fo, cluster_treatment_group, target_n, n, optimised_fo_days_deployed,optimised_outcome_n, optimised_outcome_n) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1BH4vi0MZoFPH6FzZxaDE3NkeRfCAaWWkgsS4EhIMzZk/edit#gid=1174432178",
              sheet = "Optimising FO Allocation",
              range = "A2",
              col_names = F,
              reformat = F)

```

# 4. By FO

## 4.1 Quantity & Locations

```{r}

# Recruits per FO
survey_locations %>% 
  group_by(en_name) %>% 
  summarise(number_completed_surveys = n()) %>% 
  arrange(number_completed_surveys)

# Recruits outside of clusters by FO
survey_locations %>% 
  group_by(en_name) %>% 
  summarise(outside_cluster_n = sum(!within_cluster),
            mean_accuracy = mean(geopoint_endline.Accuracy)) %>% 
  arrange(desc(outside_cluster_n))

# Surveys far away from initial baseline survey
merged_survey_locations_df %>% 
  group_by(en_name_2) %>%
  summarise(mean_distance_to_baseline = mean(distance_meters), median_distance_to_baseline = median(distance_meters)) %>% 
  arrange(desc(median_distance_to_baseline))

```

## 4.2 Duration distribution of recruitment survey

```{r}

# Summary Stats on Duration in Minutes 
temp_df <- endline_data %>% 
  filter(!is.na(source_cred_1)) %>% 
  group_by(en_name) %>% 
  summarise(duration_mean = round(mean(duration/60)), 
            duration_median = round(median(duration/60)),
            duration_max = round(max(duration/60)), 
            duration_min = round(min(duration/60)), 
            numer_outliers_over = sum(zscore_duration > 2), 
            numer_outliers_under = sum(zscore_duration < -2)) %>% 
  arrange(duration_min)
temp_df

temp_df <- endline_data %>% 
  filter(!is.na(source_cred_1)) %>% 
  filter(submission_date_dateonly >= as_date("2024-06-19")) %>%
  mutate(duration = duration/60) %>% 
  dplyr::select(id, en_name, duration, endtime)
temp_df

# Is there any surveys that ended much later than they started?
endline_data %>% 
  filter(!is.na(source_cred_1)) %>% 
   mutate(diff_start_end_mins = endtime - starttime) %>% 
   group_by(en_name) %>% 
   summarise(diff_start_end_mins_mean = round(mean(diff_start_end_mins)), 
            diff_start_end_mins_median = round(median(diff_start_end_mins)),
            diff_start_end_mins_max = round(max(diff_start_end_mins)), 
            diff_start_end_mins_min = round(min(diff_start_end_mins)), 
            numer_outliers_over = sum(zscore_duration > 2), 
            numer_outliers_under = sum(zscore_duration < -2)) %>% 
  arrange(diff_start_end_mins_mean)


#### HOW MANY FOUND A NEW PARTICIPANT IN UNDER 5 MINUTES

# Assuming df has columns en_name, starttime, and endtime, and they are already in datetime format
results <- endline_data %>%
  mutate(day = as.Date(starttime)) %>%
  group_by(en_name, day) %>%
  # Perform a self-join to compare each starttime with endtimes in the group
  do({
    self_join <- full_join(., ., by = c("en_name", "day"))
    # Filter out the same row matches
    self_join <- filter(self_join, starttime.x < endtime.y)
    # Check if starttime is within less than 10 minutes of endtime
    self_join <- mutate(self_join, within_10 = abs(difftime(starttime.x, endtime.y, units = "mins")) < 10)
    # Now summarize and count the true values of within_10
    summarise(self_join, count = sum(within_10, na.rm = TRUE))
  }) %>%
  ungroup()

# View the results
results %>% 
  filter(day >= as_date("2023-11-16")) %>% 
  arrange(desc(count))


temp_df <- endline_data %>% 
  mutate(day = as.Date(starttime)) %>% 
  filter(en_name %in% results$en_name[results$count > 0] & day %in% results$day[results$count > 0]) %>% 
  dplyr::select(en_name, starttime, endtime, duration, month, day, hour, minute, id, audio) %>% 
  arrange(en_name, day, hour, minute)
temp_df

```


## 4.3 Arbitrary Responses

```{r}

# How often does this happen per FO
arbitrary_responses_df %>%
  #filter(endline_data$submission_date_dateonly >= "2023-11-23") %>% 
  group_by(en_name) %>% 
  summarise(mean_number_groups_single_response = mean(number_groups_single_resp),
          mean_share_groups_single_response = mean(share_groups_single_resp)) %>% 
  arrange(desc(mean_number_groups_single_response))


# Which groups are the ones skipped most by FO
temp_df <- arbitrary_responses_df %>%
  # Separate Groups into rows
  separate_rows(groups_single_resp, sep = ", ") %>%
  # Count occurrences
  count(en_name, groups_single_resp) %>%
  # Convert to required format
  group_by(en_name) %>%
  summarise(GroupSummary = paste0(groups_single_resp, " (", n, ")", collapse = ", "))
temp_df

```

## 4.4 Breakdown of Demographics (check for imbalances)

```{r}
# Compute Summary Stats for the key demographics
# lapply(apply(temp_data_demographics, MARGIN = 2, FUN = function(i) tblFun_II(i, endline_data$en_name)), t)
```

## 4.5 Outliers, “don’t know” patterns, + time's taken to respond

```{r}

# Don't Know patterns by FO
temp_df <- cbind(en_name = endline_data$en_name, data.frame(endline_data[,!colnames(endline_data) %in% c("en_name")] == 88))
temp_df$row_sum <- rowSums(temp_df[,-1], na.rm = T)
temp_df %>% 
  filter(endline_data$submission_date_dateonly >= as_date("2023-11-16")) %>% 
  group_by(en_name) %>% 
  summarise(n(), dont_know_sum = sum(row_sum), dont_know_mean = mean(row_sum), dont_know_max =max(row_sum)) %>% 
  arrange(desc(dont_know_mean))

# Refuse to answer patterns by FO
temp_df <- cbind(en_name = endline_data$en_name, data.frame(endline_data[,!colnames(endline_data) %in% c("en_name")] == 99))
temp_df$row_sum <- rowSums(temp_df[,-1], na.rm = T)
temp_df %>% 
  filter(endline_data$submission_date_dateonly >= as_date("2023-11-16")) %>% 
  group_by(en_name) %>% 
  summarise(n(), refuse_sum = sum(row_sum), refuse_mean = mean(row_sum), refuse_max = max(row_sum)) %>% 
  arrange(desc(refuse_mean))


# Remove clutter
rm(temp_df)

```

## 4.6 Do FOs submit survey responses in a timely manner or is there big gaps

```{r}

# How much was the delay of uploading
endline_data$upload_delay_mins <- as.numeric(round((endline_data$SubmissionDate - endline_data$endtime)/60, 1))

# By FO
endline_data %>% 
  group_by(en_name) %>% 
  summarise(mean_upload_delay_mins = mean(upload_delay_mins)) %>% 
  arrange(desc(mean_upload_delay_mins))

# By County (maybe there is some network issues)
endline_data %>% 
  group_by(county_name) %>% 
  summarise(mean_upload_delay_mins = mean(upload_delay_mins))


```

## 4.7 Performance Dashboard

```{r}

### Performance - Quantity
en_performance_quantity <- endline_data %>% 
  group_by(en_name) %>% 
  summarise(number_completed_surveys = n()) %>% 
  arrange(number_completed_surveys)

```


```{r}

### Performance - Quality Audited

# Read in all audits/backchecks
regular_audio_audits <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=2072692660",
           sheet = "Regular_Audio_Audits")
manual_audio_audits <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=2072692660",
           sheet = "Manual_Audio_Audits") %>% 
  filter(!is.na(audio))
backchecks <- read_sheet("https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=2072692660",
           sheet = "Backchecks")


# overview table
all_checks_outcomes <- data.frame(rbind(cbind(regular_audio_audits$en_name, regular_audio_audits$SubmissionDate,regular_audio_audits$`Final Outcome (Box Ticking)`), cbind(manual_audio_audits$en_name, manual_audio_audits$SubmissionDate, manual_audio_audits$`Final Outcome (Box Ticking)`), cbind(backchecks$en_name, backchecks$SubmissionDate, backchecks$`Final Outcome (Box Ticking)`)))
colnames(all_checks_outcomes) <- c("en_name", "Date", "Outcome")
all_checks_outcomes$Date <- as_date(all_checks_outcomes$Date)
all_checks_outcomess <- all_checks_outcomes %>% arrange(en_name, Date)

# Which FOs to keep for the remainder of VCF
fos_that_cheated <- unique(all_checks_outcomes$en_name[all_checks_outcomes$Outcome == "Fraud" & !is.na(all_checks_outcomes$Outcome)])

en_performance_quality_audited <- all_checks_outcomes %>% 
  filter(!(en_name %in% fos_that_cheated)) %>% 
  group_by(en_name, Outcome) %>% 
  summarise(n = n()) %>% 
  mutate(score = case_when(Outcome == "All good" ~ n*1,
                           Outcome == "Poor Quality (not fraud)" ~ n*-1,
                           Outcome == "Fraud" ~ -Inf)) %>% 
  summarise(audit_score = sum(score, na.rm = T)) %>% 
  arrange(desc(audit_score))


en_performance_quality_audited_additional <- all_checks_outcomes %>% 
  filter(en_name %in% c("Ruth Kilobi", "Benjamin Thoge", "Esther Adhiambo")) %>% 
  group_by(en_name, Outcome) %>% 
  summarise(n = n()) %>% 
  mutate(score = case_when(Outcome == "All good" ~ n*1,
                           Outcome == "Poor Quality (not fraud)" ~ n*-1,
                           Outcome == "Fraud" ~ -3)) %>% 
  summarise(audit_score = sum(score, na.rm = T)) %>% 
  arrange(desc(audit_score))

en_performance_quality_audited <- rbind(en_performance_quality_audited, en_performance_quality_audited_additional)
  
```

```{r}

### Performance - Duration

# How often did they conduct a survey below 20 minutes
performance_too_short <- endline_data %>% 
  group_by(en_name) %>% 
  summarise(below_20min = mean(duration/60 < 20)) %>% 
  arrange(desc(below_20min))

```

```{r}

### Performance - Arbitrary Responses

# How many responses were exactly the same across groups
performance_arbitrary <- arbitrary_responses_df %>%
  group_by(en_name) %>% 
  summarise(mean_number_groups_single_response = mean(number_groups_single_resp),
          mean_share_groups_single_response = mean(share_groups_single_resp)) %>% 
  arrange(desc(mean_number_groups_single_response))

```

```{r}

### Merge and Upload

which_fos_to_keep <- purrr::reduce(list(en_performance_quality_audited, en_performance_quantity, performance_too_short, performance_arbitrary),dplyr::left_join, by = 'en_name')
  
final_score <- which_fos_to_keep %>% 
  mutate_at(c("number_completed_surveys", "audit_score", "below_20min", "mean_number_groups_single_response", "mean_share_groups_single_response"), ~(scales::rescale(.) %>% as.vector)) %>% 
  mutate(final_score = (number_completed_surveys + audit_score - below_20min - mean_share_groups_single_response), .after = 1) %>% 
  arrange(desc(final_score)) %>% 
    pull(final_score)

which_fos_to_keep %>% 
  mutate(final_score = round(final_score*100)) %>%
  mutate(share_surveys_below_20mins = paste0(round(below_20min*100), "%")) %>% 
  mutate(mean_share_qgroups_abitrary_response = paste0(round(mean_share_groups_single_response*100), "%")) %>% 
  dplyr::select(en_name, number_completed_surveys, audit_score, share_surveys_below_20mins, mean_share_qgroups_abitrary_response, final_score) %>% 
  arrange(desc(final_score)) %>% 
  range_write(ss = "https://docs.google.com/spreadsheets/d/1WN1L_yK1q3dTJC8pgcA7BcWwk8QBoLJ4eMfntZA2x8A/edit#gid=2072692660",
              sheet = "FO_Performance_Dashboard",
              reformat = F,
              col_names = F,
              range = "A2")
  
```

