---
title: "HPV_RCT_Twitter_Social_Listening"
author: "Jonathan Karl"
date: '2023-04-04'
output: html_document
---

# Set up libraries

```{r, message = F}

# Prevent scientific notation
rm(list = ls())
knitr::opts_knit$set(options(scipen=999))

# Install and load all the packages that will be used for analysis
pkgs <- c("tidyverse", "lubridate", "openxlsx", "academictwitteR", "wordcloud")

miss_pkgs <- pkgs[!pkgs %in% installed.packages()[,1]] # vector of missing packages
if(length(miss_pkgs)>0){
  install.packages(miss_pkgs)
}
invisible(lapply(pkgs,library,character.only=TRUE))

# Clear our memory by removing objects that are no longer needed.
rm(miss_pkgs, pkgs)
```

# Helper Functions

```{r}

#function to clean tweets
#part of funtion taken from: https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r another part from S. Strong (2021). 
clean_tweets <- function(x) {
                x %>%
                    #remove URLs
                    str_remove_all(" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
                    #removing code for emojis
                    str_remove_all("\\<U\\+[:digit:]{4}[:alpha:]{1}[:digit:]{1,3}[:alpha:]*[:digit:]?\\>") %>%
                    str_remove_all("\\<U\\+[:alpha:]{1,3}[:digit:]{1,3}[:alpha:]*[:digit:]?\\>") %>%
                    # Removing hashtags                
                    str_remove_all("#[[:word:]]+") %>% 
                    # Remove numbers that stand alone 
                    str_remove_all("[0-9]*") %>% 
                    #changing code for & symbol to 'and'
                    str_replace_all("&amp;", "and") %>%
                    #removing retweet abbreviation
                    str_remove_all("^RT:? ") %>%
                    #removing punctuation
                    str_remove_all("[[:punct:]]") %>%
                    #remove mentions
                    str_remove_all("@[[:word:]]+") %>%
                    #replace newline characters with space
                    str_replace_all("\\\n", " ") %>%
                    #make everything lowercase
                    str_to_lower() %>%
                    #remove trailing whitespace
                    str_trim("both") %>% 
                    # Remove double spaces
                    str_squish()
}

```

# Prep Twitter Scraper

```{r}
########## Academic Twitter Scraping
set_bearer()
get_bearer()
```

# Scrape Data

```{r}

# Set up keywords to scrape
keywords <- c("HPV", "HPV vaccine", "chanjo ya HPV")

# Build test Query
query_test <- build_query(query = keywords, is_retweet = F, country = "KE", lang = "en")

# Scrapping twitter using query and given time frame
tweets_temp <- get_all_tweets(
  query = query_test,
  bearer_token = get_bearer(),
  start_tweets = "2021-01-01T00:00:00Z",
  end_tweets = "2023-04-03T11:59:00Z",
  n = 10000)

#tweets_temp$text

```

# Clean tweets

```{r}

# Overall cleaning
tweets_clean <- clean_tweets(tweets_temp$text)


# Remove the obvious words
tweets_clean_lean <- str_remove_all(tweets_clean, "HPV|hpv|[v|V]accine|cancer|cervical")

```


# Wordclouds

```{r}

wordcloud(tweets_clean_lean)

```

